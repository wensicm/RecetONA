{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c088ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy (from -r /home/wencm/RecetONA/requirements.txt (line 1))\n",
      "  Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pandas (from -r /home/wencm/RecetONA/requirements.txt (line 2))\n",
      "  Using cached pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Collecting openpyxl (from -r /home/wencm/RecetONA/requirements.txt (line 3))\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting openai (from -r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached openai-2.20.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->-r /home/wencm/RecetONA/requirements.txt (line 2))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting et-xmlfile (from openpyxl->-r /home/wencm/RecetONA/requirements.txt (line 3))\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached jiter-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sniffio (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r /home/wencm/RecetONA/requirements.txt (line 4))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r /home/wencm/RecetONA/requirements.txt (line 2))\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached openai-2.20.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, sniffio, six, numpy, jiter, idna, h11, et-xmlfile, distro, certifi, annotated-types, typing-inspection, python-dateutil, pydantic-core, openpyxl, httpcore, anyio, pydantic, pandas, httpx, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [openai]21/22\u001b[0m [openai]c]teutil]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.12.1 certifi-2026.1.4 distro-1.9.0 et-xmlfile-2.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jiter-0.13.0 numpy-2.4.2 openai-2.20.0 openpyxl-3.1.5 pandas-3.0.0 pydantic-2.12.5 pydantic-core-2.41.5 python-dateutil-2.9.0.post0 six-1.17.0 sniffio-1.3.1 tqdm-4.67.3 typing-extensions-4.15.0 typing-inspection-0.4.2\n",
      "Requirements instalados en: /home/wencm/RecetONA/lib\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path('/home/wencm/RecetONA')\n",
    "REQ_PATH = BASE_DIR / 'requirements.txt'\n",
    "LIB_DIR = BASE_DIR / 'lib'\n",
    "LIB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "subprocess.check_call([\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'pip',\n",
    "    'install',\n",
    "    '--upgrade',\n",
    "    '--target',\n",
    "    str(LIB_DIR),\n",
    "    '-r',\n",
    "    str(REQ_PATH),\n",
    "])\n",
    "\n",
    "print(f'Requirements instalados en: {LIB_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce49d4",
   "metadata": {},
   "source": [
    "# Mercadona RAG Notebook\n",
    "\n",
    "Este notebook hace RAG sobre `mercadona_data.xlsx`, recupera filas relevantes y llama a OpenAI para responder usando solo ese contexto.\n",
    "\n",
    "## Requisitos\n",
    "- Tener `OPENAI_API_KEY` en variables de entorno.\n",
    "- Tener `openai`, `pandas`, `numpy`, `openpyxl` instalados en `/home/wencm/RecetONA/lib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcf8ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel: /home/wencm/RecetONA/mercadona_data.xlsx\n",
      "Cache: /home/wencm/RecetONA/rag_cache\n",
      "Python: /home/wencm/WhaRAGBot/.venv/bin/python\n",
      "OPENAI_API_KEY presente: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "LIB_DIR = Path('/home/wencm/RecetONA/lib')\n",
    "if str(LIB_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(LIB_DIR))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "BASE_DIR = Path('/home/wencm/RecetONA')\n",
    "EXCEL_PATH = BASE_DIR / 'mercadona_data.xlsx'\n",
    "CACHE_DIR = BASE_DIR / 'rag_cache'\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "ENV_PATH = BASE_DIR / '.env'\n",
    "\n",
    "CHUNKS_CSV = CACHE_DIR / 'chunks.csv'\n",
    "EMB_NPY = CACHE_DIR / 'embeddings.npy'\n",
    "\n",
    "EMBED_MODEL = 'text-embedding-3-small'\n",
    "CHAT_MODEL = 'gpt-4.1-mini'\n",
    "\n",
    "def load_env_file(path: Path) -> bool:\n",
    "    if not path.exists():\n",
    "        return False\n",
    "\n",
    "    loaded_any = False\n",
    "    for raw_line in path.read_text(encoding='utf-8').splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith('#') or '=' not in line:\n",
    "            continue\n",
    "\n",
    "        key, value = line.split('=', 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "\n",
    "        if (value.startswith('\"') and value.endswith('\"')) or (value.startswith(\"'\") and value.endswith(\"'\")):\n",
    "            value = value[1:-1]\n",
    "\n",
    "        if key and key not in os.environ:\n",
    "            os.environ[key] = value\n",
    "            loaded_any = True\n",
    "\n",
    "    return loaded_any\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    loaded = load_env_file(ENV_PATH)\n",
    "    if loaded:\n",
    "        print(f'Variables cargadas desde: {ENV_PATH}')\n",
    "\n",
    "print(f'Excel: {EXCEL_PATH}')\n",
    "print(f'Cache: {CACHE_DIR}')\n",
    "print(f'Python: {sys.executable}')\n",
    "print('OPENAI_API_KEY presente:', bool(os.getenv('OPENAI_API_KEY')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49c89ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 4553\n",
      "Chunks guardados en: /home/wencm/RecetONA/rag_cache/chunks.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_idx</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>subsubcategory</th>\n",
       "      <th>packaging</th>\n",
       "      <th>unit_size</th>\n",
       "      <th>size_format</th>\n",
       "      <th>price_unit</th>\n",
       "      <th>price_bulk</th>\n",
       "      <th>ingredientes</th>\n",
       "      <th>alergenos</th>\n",
       "      <th>nutrition_ocr_text</th>\n",
       "      <th>text</th>\n",
       "      <th>lexical_text</th>\n",
       "      <th>ingredient_search_text</th>\n",
       "      <th>ingredient_desc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4241.0</td>\n",
       "      <td>Aceite de oliva 0,4º Hacendado</td>\n",
       "      <td>Aceite, especias y salsas</td>\n",
       "      <td>Aceite, vinagre y sal</td>\n",
       "      <td>Aceite de oliva</td>\n",
       "      <td>Garrafa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>l</td>\n",
       "      <td>19.75</td>\n",
       "      <td>3.95</td>\n",
       "      <td>Ingredientes: Aceite de oliva refinado y Aceit...</td>\n",
       "      <td>x99.</td>\n",
       "      <td>Aceitede Oliva | Que contiene exclusivamente |...</td>\n",
       "      <td>Producto: Aceite de oliva 0,4º Hacendado\\nID p...</td>\n",
       "      <td>Aceite de oliva 0,4º Hacendado Aceite, especia...</td>\n",
       "      <td>Aceite de oliva 0,4º Hacendado Aceite, especia...</td>\n",
       "      <td>Ingredientes: Aceite de oliva refinado y Aceit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>Aceite de oliva 0,4º Hacendado</td>\n",
       "      <td>Aceite, especias y salsas</td>\n",
       "      <td>Aceite, vinagre y sal</td>\n",
       "      <td>Aceite de oliva</td>\n",
       "      <td>Botella</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.10</td>\n",
       "      <td>Ingredientes: Aceite de oliva refinado y Aceit...</td>\n",
       "      <td>x99.</td>\n",
       "      <td>INGREDIENTES | CONSERVACION | Aceite de oliva ...</td>\n",
       "      <td>Producto: Aceite de oliva 0,4º Hacendado\\nID p...</td>\n",
       "      <td>Aceite de oliva 0,4º Hacendado Aceite, especia...</td>\n",
       "      <td>Aceite de oliva 0,4º Hacendado Aceite, especia...</td>\n",
       "      <td>Ingredientes: Aceite de oliva refinado y Aceit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_idx  product_id                    product_name  \\\n",
       "0        0      4241.0  Aceite de oliva 0,4º Hacendado   \n",
       "1        1      4240.0  Aceite de oliva 0,4º Hacendado   \n",
       "\n",
       "                    category            subcategory   subsubcategory  \\\n",
       "0  Aceite, especias y salsas  Aceite, vinagre y sal  Aceite de oliva   \n",
       "1  Aceite, especias y salsas  Aceite, vinagre y sal  Aceite de oliva   \n",
       "\n",
       "  packaging  unit_size size_format  price_unit  price_bulk  \\\n",
       "0   Garrafa        5.0           l       19.75        3.95   \n",
       "1   Botella        1.0           l        4.10        4.10   \n",
       "\n",
       "                                        ingredientes alergenos  \\\n",
       "0  Ingredientes: Aceite de oliva refinado y Aceit...      x99.   \n",
       "1  Ingredientes: Aceite de oliva refinado y Aceit...      x99.   \n",
       "\n",
       "                                  nutrition_ocr_text  \\\n",
       "0  Aceitede Oliva | Que contiene exclusivamente |...   \n",
       "1  INGREDIENTES | CONSERVACION | Aceite de oliva ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Producto: Aceite de oliva 0,4º Hacendado\\nID p...   \n",
       "1  Producto: Aceite de oliva 0,4º Hacendado\\nID p...   \n",
       "\n",
       "                                        lexical_text  \\\n",
       "0  Aceite de oliva 0,4º Hacendado Aceite, especia...   \n",
       "1  Aceite de oliva 0,4º Hacendado Aceite, especia...   \n",
       "\n",
       "                              ingredient_search_text  \\\n",
       "0  Aceite de oliva 0,4º Hacendado Aceite, especia...   \n",
       "1  Aceite de oliva 0,4º Hacendado Aceite, especia...   \n",
       "\n",
       "                                ingredient_desc_text  \n",
       "0  Ingredientes: Aceite de oliva refinado y Aceit...  \n",
       "1  Ingredientes: Aceite de oliva refinado y Aceit...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _clean(v):\n",
    "    if pd.isna(v):\n",
    "        return ''\n",
    "    s = str(v).strip()\n",
    "    return '' if s.lower() == 'nan' else s\n",
    "\n",
    "def _numeric(v):\n",
    "    if pd.isna(v):\n",
    "        return ''\n",
    "    try:\n",
    "        f = float(v)\n",
    "    except Exception:\n",
    "        return _clean(v)\n",
    "    return f'{f:g}'\n",
    "\n",
    "def build_row_text(row):\n",
    "    parts = []\n",
    "    parts.append(f\"Producto: {_clean(row.get('product_name'))}\")\n",
    "    parts.append(f\"ID producto: {_numeric(row.get('product_id'))}\")\n",
    "    parts.append(f\"Categoria: {_clean(row.get('category'))} > {_clean(row.get('subcategory'))} > {_clean(row.get('subsubcategory'))}\")\n",
    "    parts.append(f\"Formato: {_clean(row.get('packaging'))}, {_numeric(row.get('unit_size'))} {_clean(row.get('size_format'))}\")\n",
    "    parts.append(f\"Precio unidad: {_numeric(row.get('price_unit'))} EUR\")\n",
    "    parts.append(f\"Precio por volumen/peso: {_numeric(row.get('price_bulk'))} EUR\")\n",
    "    parts.append(f\"Ingredientes: {_clean(row.get('Ingredientes'))}\")\n",
    "    parts.append(f\"Alergenos: {_clean(row.get('Alérgenos'))}\")\n",
    "    parts.append(\n",
    "        'Nutricion por 100g/ml: '\n",
    "        f\"kJ={_numeric(row.get('nutrition_kj_100'))}; \"\n",
    "        f\"kcal={_numeric(row.get('nutrition_kcal_100'))}; \"\n",
    "        f\"grasas={_numeric(row.get('nutrition_fat_g_100'))} g; \"\n",
    "        f\"saturadas={_numeric(row.get('nutrition_saturates_g_100'))} g; \"\n",
    "        f\"hidratos={_numeric(row.get('nutrition_carbs_g_100'))} g; \"\n",
    "        f\"azucares={_numeric(row.get('nutrition_sugars_g_100'))} g; \"\n",
    "        f\"proteinas={_numeric(row.get('nutrition_protein_g_100'))} g; \"\n",
    "        f\"sal={_numeric(row.get('nutrition_salt_g_100'))} g\"\n",
    "    )\n",
    "    parts.append(f\"Imagen principal: {_clean(row.get('thumbnail_url'))}\")\n",
    "    parts.append(f\"Imagenes: {_clean(row.get('photo_urls'))}\")\n",
    "    return '\\n'.join(parts)\n",
    "\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "print('Filas:', len(df))\n",
    "\n",
    "chunks = pd.DataFrame({\n",
    "    'row_idx': np.arange(len(df), dtype=int),\n",
    "    'product_id': df.get('product_id'),\n",
    "    'product_name': df.get('product_name'),\n",
    "    'category': df.get('category'),\n",
    "    'subcategory': df.get('subcategory'),\n",
    "    'subsubcategory': df.get('subsubcategory'),\n",
    "    'packaging': df.get('packaging'),\n",
    "    'unit_size': df.get('unit_size'),\n",
    "    'size_format': df.get('size_format'),\n",
    "    'price_unit': df.get('price_unit'),\n",
    "    'price_bulk': df.get('price_bulk'),\n",
    "    'ingredientes': df.get('Ingredientes'),\n",
    "    'alergenos': df.get('Alérgenos'),\n",
    "    'nutrition_ocr_text': df.get('nutrition_ocr_text'),\n",
    "    'text': [build_row_text(r) for _, r in df.iterrows()],\n",
    "})\n",
    "\n",
    "chunks['lexical_text'] = (\n",
    "    chunks['product_name'].fillna('').astype(str) + ' ' +\n",
    "    chunks['category'].fillna('').astype(str) + ' ' +\n",
    "    chunks['subcategory'].fillna('').astype(str) + ' ' +\n",
    "    chunks['subsubcategory'].fillna('').astype(str) + ' ' +\n",
    "    chunks['ingredientes'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "# Ingredient-focused name/index fields (high precision)\n",
    "chunks['ingredient_search_text'] = (\n",
    "    chunks['product_name'].fillna('').astype(str) + ' ' +\n",
    "    chunks['category'].fillna('').astype(str) + ' ' +\n",
    "    chunks['subcategory'].fillna('').astype(str) + ' ' +\n",
    "    chunks['subsubcategory'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "# Ingredient-focused description fields (high recall)\n",
    "chunks['ingredient_desc_text'] = (\n",
    "    chunks['ingredientes'].fillna('').astype(str) + ' ' +\n",
    "    chunks['alergenos'].fillna('').astype(str) + ' ' +\n",
    "    chunks['nutrition_ocr_text'].fillna('').astype(str).str.slice(0, 2500)\n",
    ")\n",
    "\n",
    "chunks.to_csv(CHUNKS_CSV, index=False)\n",
    "print('Chunks guardados en:', CHUNKS_CSV)\n",
    "chunks.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119990a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise RuntimeError(\n",
    "        'Falta OPENAI_API_KEY. Define la variable en tu entorno o crea '\n",
    "        f\"{ENV_PATH}\"\n",
    "        ' con una linea: OPENAI_API_KEY=tu_clave'\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def _l2_normalize(arr):\n",
    "    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "    return arr / norms\n",
    "\n",
    "def embed_texts(texts, model=EMBED_MODEL, batch_size=64):\n",
    "    vectors = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start + batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        batch_vecs = [d.embedding for d in resp.data]\n",
    "        vectors.extend(batch_vecs)\n",
    "        print(f'Embeddings: {min(start + batch_size, len(texts))}/{len(texts)}')\n",
    "    arr = np.array(vectors, dtype=np.float32)\n",
    "    return _l2_normalize(arr)\n",
    "\n",
    "def ensure_embeddings(chunks_df, force_rebuild=False):\n",
    "    if EMB_NPY.exists() and not force_rebuild:\n",
    "        emb = np.load(EMB_NPY)\n",
    "        if emb.shape[0] == len(chunks_df):\n",
    "            print('Embeddings cargados desde cache:', EMB_NPY)\n",
    "            return emb\n",
    "        print('Cache invalida por numero de filas. Se regenera.')\n",
    "\n",
    "    emb = embed_texts(chunks_df['text'].tolist())\n",
    "    np.save(EMB_NPY, emb)\n",
    "    print('Embeddings guardados en:', EMB_NPY)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307d99da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cargados desde cache: /home/wencm/RecetONA/rag_cache/embeddings.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4553, 1536)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = ensure_embeddings(chunks)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c03af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    'Eres un asistente experto en el catalogo de Mercadona. '\n",
    "    'Responde SOLO con la informacion del contexto recuperado. '\n",
    "    'Si no hay evidencia suficiente, di claramente que no aparece en el dataset. '\n",
    "    'Si la pregunta es una receta, construye una lista de compra realista para el numero de personas '\n",
    "    'usando productos presentes en el contexto, y separa siempre: '\n",
    "    '1) coste de compra real (envases completos), '\n",
    "    '2) escandallo real del plato (coste consumido). '\n",
    "    'Si falta precio de algun ingrediente, indicalo explicitamente.'\n",
    ")\n",
    "\n",
    "RECIPE_TRIGGERS = (\n",
    "    'receta', 'rehogado', 'rehogar', 'guiso', 'cocinar', 'ingredientes',\n",
    "    'personas', 'menu', 'plato', 'preparar'\n",
    ")\n",
    "\n",
    "STOPWORDS = {\n",
    "    'de', 'la', 'el', 'los', 'las', 'un', 'una', 'unos', 'unas', 'para', 'por', 'con', 'sin',\n",
    "    'que', 'del', 'al', 'en', 'y', 'o', 'a', 'se', 'su', 'sus', 'como', 'dime', 'quiero',\n",
    "    'hacer', 'preparar', 'receta', 'personas', 'plato', 'menu'\n",
    "}\n",
    "\n",
    "# Cantidades base estimadas para 4 personas (unidad compatible con dataset: kg/l/ud)\n",
    "RECIPE_REQUIREMENTS_BASE_4P = {\n",
    "    'lentejas': (0.40, 'kg'),\n",
    "    'arroz': (0.32, 'kg'),\n",
    "    'marisco': (0.45, 'kg'),\n",
    "    'caldo de marisco': (1.00, 'l'),\n",
    "    'caldo': (1.00, 'l'),\n",
    "    'tomate': (0.20, 'kg'),\n",
    "    'cebolla': (0.15, 'kg'),\n",
    "    'ajo': (0.02, 'kg'),\n",
    "    'pimiento': (0.15, 'kg'),\n",
    "    'zanahoria': (0.15, 'kg'),\n",
    "    'aceite de oliva': (0.06, 'l'),\n",
    "    'huevo': (4.00, 'ud'),\n",
    "    'huevos': (4.00, 'ud'),\n",
    "    'sal': (0.01, 'kg'),\n",
    "    'pimenton': (0.005, 'kg'),\n",
    "    'laurel': (0.002, 'kg'),\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    return ''.join(ch for ch in text if unicodedata.category(ch) != 'Mn')\n",
    "\n",
    "def _token_variants(token):\n",
    "    variants = {token}\n",
    "    if len(token) > 4 and token.endswith('es'):\n",
    "        variants.add(token[:-2])\n",
    "    if len(token) > 3 and token.endswith('s'):\n",
    "        variants.add(token[:-1])\n",
    "    return variants\n",
    "\n",
    "def tokenize(text):\n",
    "    raw = re.findall(r'[a-z0-9]+', normalize_text(text))\n",
    "    out = []\n",
    "    for tok in raw:\n",
    "        out.extend(_token_variants(tok))\n",
    "    return out\n",
    "\n",
    "def _safe_float(v):\n",
    "    try:\n",
    "        if v is None:\n",
    "            return None\n",
    "        if isinstance(v, str) and not v.strip():\n",
    "            return None\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _fmt_money(v):\n",
    "    if v is None or (isinstance(v, float) and math.isnan(v)):\n",
    "        return '-'\n",
    "    return f'{v:.2f}'\n",
    "\n",
    "def _fmt_num(v, digits=3):\n",
    "    if v is None or (isinstance(v, float) and math.isnan(v)):\n",
    "        return '-'\n",
    "    return f'{v:.{digits}f}'\n",
    "\n",
    "def _to_dim_base(value, unit):\n",
    "    value = _safe_float(value)\n",
    "    unit_n = normalize_text(unit)\n",
    "    if value is None:\n",
    "        return None, None\n",
    "\n",
    "    if unit_n == 'kg':\n",
    "        return 'mass_kg', value\n",
    "    if unit_n == 'g':\n",
    "        return 'mass_kg', value / 1000.0\n",
    "    if unit_n == 'l':\n",
    "        return 'vol_l', value\n",
    "    if unit_n == 'ml':\n",
    "        return 'vol_l', value / 1000.0\n",
    "    if unit_n in ('ud', 'u', 'unidad', 'unidades'):\n",
    "        return 'unit_ud', value\n",
    "    return None, None\n",
    "\n",
    "def parse_servings(query, default=4):\n",
    "    qn = normalize_text(query)\n",
    "    m = re.search(r'(\\d+)\\s*personas?', qn)\n",
    "    if m:\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "            if n > 0:\n",
    "                return n\n",
    "        except Exception:\n",
    "            pass\n",
    "    return default\n",
    "\n",
    "def is_recipe_query(query):\n",
    "    qn = normalize_text(query)\n",
    "    return any(trigger in qn for trigger in RECIPE_TRIGGERS)\n",
    "\n",
    "def _remove_redundant_ingredients(items):\n",
    "    normalized = [(item, normalize_text(item)) for item in items]\n",
    "    keep = []\n",
    "\n",
    "    for i, (item, norm_item) in enumerate(normalized):\n",
    "        redundant = False\n",
    "        for j, (_, norm_other) in enumerate(normalized):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if norm_item in norm_other and len(norm_item.split()) < len(norm_other.split()):\n",
    "                redundant = True\n",
    "                break\n",
    "        if not redundant:\n",
    "            keep.append(item)\n",
    "\n",
    "    return keep\n",
    "\n",
    "def _extract_json_list(text):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # 1) Try raw JSON first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "        if isinstance(obj, dict) and isinstance(obj.get('ingredients'), list):\n",
    "            return obj['ingredients']\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try to capture first JSON list in markdown or prose\n",
    "    m = re.search(r'\\[[\\s\\S]*\\]', text)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0))\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return []\n",
    "\n",
    "def _normalize_ingredient_alias(s):\n",
    "    s = normalize_text(s)\n",
    "    if not s:\n",
    "        return s\n",
    "\n",
    "    alias_rules = [\n",
    "        (r'\\bpolvos?\\s+(de|para)\\s+hornear\\b', 'gasificante reposteria'),\n",
    "        (r'\\blevadura\\s+quimica\\b', 'gasificante reposteria'),\n",
    "        (r'\\bimpulsor(?:\\s+gasificante)?\\b', 'gasificante reposteria'),\n",
    "        (r'\\bextracto\\s+de\\s+vainilla\\b', 'aroma de vainilla'),\n",
    "        (r'\\bcrema\\s+de\\s+leche\\b', 'nata'),\n",
    "        (r'\\bcrema\\s+para\\s+batir\\b', 'nata'),\n",
    "        (r'\\bnata\\s+liquida\\b', 'nata'),\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in alias_rules:\n",
    "        if re.search(pattern, s):\n",
    "            return replacement\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def _clean_ingredient_name(item):\n",
    "    s = normalize_text(item)\n",
    "    s = re.sub(r'\\([^)]*\\)', ' ', s)\n",
    "    s = re.sub(r'\\baprox(?:imadamente)?\\b', ' ', s)\n",
    "\n",
    "    # Remove leading quantity/unit chunks: \"400 g de\", \"2 dientes de\", etc.\n",
    "    s = re.sub(\n",
    "        r'^\\s*\\d+(?:[\\.,]\\d+)?\\s*(kg|g|gr|gramos?|ml|l|litros?|unidad(?:es)?|ud|dientes?|cucharad(?:a|as)|taza(?:s)?)?\\s*(de\\s+)?',\n",
    "        '',\n",
    "        s,\n",
    "    )\n",
    "    s = re.sub(r'^\\s*(unos?|unas?|un|una)\\s+', '', s)\n",
    "\n",
    "    # Remove residual quantities/units inside phrase.\n",
    "    s = re.sub(r'\\b\\d+(?:[\\.,]\\d+)?\\b', ' ', s)\n",
    "    s = re.sub(r'\\b(kg|g|gr|gramos?|ml|l|litros?|unidad(?:es)?|ud|dientes?|cucharad(?:a|as)|taza(?:s)?)\\b', ' ', s)\n",
    "\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    if s.startswith('de '):\n",
    "        s = s[3:].strip()\n",
    "\n",
    "    return _normalize_ingredient_alias(s)\n",
    "\n",
    "def infer_recipe_ingredients_llm(query, max_items=12):\n",
    "    prompt = (\n",
    "        'Extrae ingredientes para la receta solicitada. '\n",
    "        'Devuelve SOLO un JSON array de strings, sin texto adicional. '\n",
    "        'No incluyas cantidades ni unidades. '\n",
    "        'Incluye ingredientes base y condimentos habituales. '\n",
    "        'No incluyas bebidas, refrescos ni acompanamientos opcionales. '\n",
    "        f'Consulta: {query}'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = client.responses.create(\n",
    "            model=CHAT_MODEL,\n",
    "            input=prompt,\n",
    "        )\n",
    "        raw = (resp.output_text or '').strip()\n",
    "        arr = _extract_json_list(raw)\n",
    "\n",
    "        out = []\n",
    "        seen = set()\n",
    "        for x in arr:\n",
    "            if not isinstance(x, str):\n",
    "                continue\n",
    "            item = _clean_ingredient_name(x)\n",
    "            if len(item) < 2:\n",
    "                continue\n",
    "            key = normalize_text(item)\n",
    "            if not key or key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            out.append(item)\n",
    "            if len(out) >= max_items:\n",
    "                break\n",
    "\n",
    "        return out\n",
    "    except Exception as exc:\n",
    "        print(f'WARN infer_recipe_ingredients_llm fallo: {exc}')\n",
    "        return []\n",
    "\n",
    "def infer_recipe_ingredients(query):\n",
    "    if not is_recipe_query(query):\n",
    "        return []\n",
    "\n",
    "    inferred = infer_recipe_ingredients_llm(query)\n",
    "    if inferred:\n",
    "        return _remove_redundant_ingredients(inferred)\n",
    "\n",
    "    # Fallback sin reglas de plato concreto\n",
    "    tokens = [\n",
    "        t for t in tokenize(query)\n",
    "        if len(t) > 2 and t not in STOPWORDS and not t.isdigit()\n",
    "    ]\n",
    "    return _remove_redundant_ingredients(tokens[:8])\n",
    "\n",
    "def _ingredient_requirement_key(ingredient):\n",
    "    ing_n = normalize_text(ingredient)\n",
    "    keys = sorted(RECIPE_REQUIREMENTS_BASE_4P.keys(), key=len, reverse=True)\n",
    "    for key in keys:\n",
    "        if key in ing_n:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "def estimate_required_quantity(ingredient, servings=4):\n",
    "    key = _ingredient_requirement_key(ingredient)\n",
    "    if key is None:\n",
    "        return 1.0 * (servings / 4.0), 'ud', 'fallback'\n",
    "\n",
    "    base_qty, base_unit = RECIPE_REQUIREMENTS_BASE_4P[key]\n",
    "    factor = servings / 4.0\n",
    "    return base_qty * factor, base_unit, key\n",
    "\n",
    "def expand_subqueries(query, recipe_mode='auto'):\n",
    "    subqueries = [query]\n",
    "    inferred_ingredients = []\n",
    "\n",
    "    use_recipe = (recipe_mode is True) or (recipe_mode == 'auto' and is_recipe_query(query))\n",
    "    if use_recipe:\n",
    "        inferred_ingredients = infer_recipe_ingredients(query)\n",
    "        for ingredient in inferred_ingredients:\n",
    "            subqueries.append(f'{ingredient} mercadona precio')\n",
    "\n",
    "    return subqueries, inferred_ingredients\n",
    "\n",
    "# General index (RAG context)\n",
    "DOC_TOKENS = [tokenize(text) for text in chunks['lexical_text'].fillna('')]\n",
    "POSTINGS = defaultdict(list)\n",
    "DOC_FREQ = defaultdict(int)\n",
    "N_DOCS = len(DOC_TOKENS)\n",
    "\n",
    "for doc_idx, toks in enumerate(DOC_TOKENS):\n",
    "    counts = Counter(toks)\n",
    "    for tok, tf in counts.items():\n",
    "        POSTINGS[tok].append((doc_idx, tf))\n",
    "        DOC_FREQ[tok] += 1\n",
    "\n",
    "# Ingredient-focused indexes\n",
    "# 1) Name/category index (precision)\n",
    "ING_NAME_DOC_TOKENS = [tokenize(text) for text in chunks['ingredient_search_text'].fillna('')]\n",
    "ING_NAME_TOKEN_SETS = [set(toks) for toks in ING_NAME_DOC_TOKENS]\n",
    "ING_NAME_POSTINGS = defaultdict(list)\n",
    "ING_NAME_DOC_FREQ = defaultdict(int)\n",
    "\n",
    "for doc_idx, toks in enumerate(ING_NAME_DOC_TOKENS):\n",
    "    counts = Counter(toks)\n",
    "    for tok, tf in counts.items():\n",
    "        ING_NAME_POSTINGS[tok].append((doc_idx, tf))\n",
    "        ING_NAME_DOC_FREQ[tok] += 1\n",
    "\n",
    "# 2) Description/ingredients index (recall)\n",
    "ING_DESC_DOC_TOKENS = [tokenize(text) for text in chunks['ingredient_desc_text'].fillna('')]\n",
    "ING_DESC_TOKEN_SETS = [set(toks) for toks in ING_DESC_DOC_TOKENS]\n",
    "ING_DESC_POSTINGS = defaultdict(list)\n",
    "ING_DESC_DOC_FREQ = defaultdict(int)\n",
    "\n",
    "for doc_idx, toks in enumerate(ING_DESC_DOC_TOKENS):\n",
    "    counts = Counter(toks)\n",
    "    for tok, tf in counts.items():\n",
    "        ING_DESC_POSTINGS[tok].append((doc_idx, tf))\n",
    "        ING_DESC_DOC_FREQ[tok] += 1\n",
    "\n",
    "def _tfidf_scores_index(q_tokens, postings, doc_freq):\n",
    "    if not q_tokens:\n",
    "        return np.zeros(N_DOCS, dtype=np.float32)\n",
    "\n",
    "    q_counts = Counter(q_tokens)\n",
    "    scores = np.zeros(N_DOCS, dtype=np.float32)\n",
    "\n",
    "    for tok, qtf in q_counts.items():\n",
    "        rows = postings.get(tok)\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        idf = math.log((N_DOCS + 1) / (doc_freq[tok] + 1)) + 1.0\n",
    "        q_weight = 1.0 + math.log(qtf)\n",
    "        for doc_i, tf in rows:\n",
    "            scores[doc_i] += q_weight * (1.0 + math.log(tf)) * idf\n",
    "\n",
    "    max_score = float(scores.max())\n",
    "    if max_score > 0:\n",
    "        scores /= max_score\n",
    "    return scores\n",
    "\n",
    "def lexical_scores(query):\n",
    "    q_tokens = [t for t in tokenize(query) if len(t) > 2 and t not in STOPWORDS]\n",
    "    return _tfidf_scores_index(q_tokens, POSTINGS, DOC_FREQ)\n",
    "\n",
    "def lexical_scores_ingredient(query):\n",
    "    q_tokens = [t for t in tokenize(query) if len(t) > 2 and t not in STOPWORDS]\n",
    "    name_scores = _tfidf_scores_index(q_tokens, ING_NAME_POSTINGS, ING_NAME_DOC_FREQ)\n",
    "    desc_scores = _tfidf_scores_index(q_tokens, ING_DESC_POSTINGS, ING_DESC_DOC_FREQ)\n",
    "\n",
    "    # Name/category drives precision; ingredients/description improves recall for indirect product names.\n",
    "    return 0.80 * name_scores + 0.20 * desc_scores\n",
    "\n",
    "def semantic_scores_batch(queries):\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=queries)\n",
    "    all_scores = []\n",
    "    for item in resp.data:\n",
    "        q_vec = np.array(item.embedding, dtype=np.float32)\n",
    "        q_vec = q_vec / (np.linalg.norm(q_vec) + 1e-12)\n",
    "        sem = embeddings @ q_vec\n",
    "        all_scores.append((sem + 1.0) / 2.0)\n",
    "    return all_scores\n",
    "\n",
    "def _ingredient_token_masks(ingredient):\n",
    "    tokens = [t for t in tokenize(ingredient) if t not in STOPWORDS and len(t) > 2]\n",
    "    if not tokens:\n",
    "        zeros = np.zeros(N_DOCS, dtype=bool)\n",
    "        return zeros, zeros\n",
    "\n",
    "    any_mask = np.zeros(N_DOCS, dtype=bool)\n",
    "    all_mask = np.zeros(N_DOCS, dtype=bool)\n",
    "\n",
    "    for doc_i, tokset in enumerate(ING_NAME_TOKEN_SETS):\n",
    "        present = [tok in tokset for tok in tokens]\n",
    "        if any(present):\n",
    "            any_mask[doc_i] = True\n",
    "        if all(present):\n",
    "            all_mask[doc_i] = True\n",
    "\n",
    "    return any_mask, all_mask\n",
    "\n",
    "def retrieve_hybrid(query, top_k=20, alpha=0.65, recipe_mode='auto'):\n",
    "    subqueries, inferred_ingredients = expand_subqueries(query, recipe_mode=recipe_mode)\n",
    "    sem_scores_list = semantic_scores_batch(subqueries)\n",
    "\n",
    "    hybrid_best = np.zeros(N_DOCS, dtype=np.float32)\n",
    "    semantic_best = np.zeros(N_DOCS, dtype=np.float32)\n",
    "    lexical_best = np.zeros(N_DOCS, dtype=np.float32)\n",
    "\n",
    "    for subquery, sem_scores in zip(subqueries, sem_scores_list):\n",
    "        lex_scores = lexical_scores(subquery)\n",
    "        hybrid_scores = alpha * sem_scores + (1.0 - alpha) * lex_scores\n",
    "\n",
    "        hybrid_best = np.maximum(hybrid_best, hybrid_scores)\n",
    "        semantic_best = np.maximum(semantic_best, sem_scores)\n",
    "        lexical_best = np.maximum(lexical_best, lex_scores)\n",
    "\n",
    "    top_idx = np.argsort(-hybrid_best)[:top_k]\n",
    "\n",
    "    hits = chunks.iloc[top_idx].copy()\n",
    "    hits['score'] = hybrid_best[top_idx]\n",
    "    hits['score_semantic'] = semantic_best[top_idx]\n",
    "    hits['score_lexical'] = lexical_best[top_idx]\n",
    "\n",
    "    return hits.sort_values('score', ascending=False).reset_index(drop=True), subqueries, inferred_ingredients\n",
    "\n",
    "def retrieve(query, top_k=8, mode='hybrid', alpha=0.65, recipe_mode='auto'):\n",
    "    mode = mode.lower()\n",
    "\n",
    "    if mode == 'semantic':\n",
    "        sem_scores = semantic_scores_batch([query])[0]\n",
    "        top_idx = np.argsort(-sem_scores)[:top_k]\n",
    "        hits = chunks.iloc[top_idx].copy()\n",
    "        hits['score'] = sem_scores[top_idx]\n",
    "        return hits.sort_values('score', ascending=False).reset_index(drop=True), [query], []\n",
    "\n",
    "    if mode == 'hybrid':\n",
    "        return retrieve_hybrid(query, top_k=top_k, alpha=alpha, recipe_mode=recipe_mode)\n",
    "\n",
    "    raise ValueError(\"mode debe ser 'semantic' o 'hybrid'\")\n",
    "\n",
    "def retrieve_products_for_ingredient(ingredient, top_n=10, alpha=0.35, recipe_query=None):\n",
    "    ingredient_query = normalize_text(ingredient).strip()\n",
    "    context_query = normalize_text(recipe_query).strip() if recipe_query else ''\n",
    "\n",
    "    cols = [\n",
    "        'row_idx', 'product_id', 'product_name', 'category', 'subcategory',\n",
    "        'packaging', 'unit_size', 'size_format', 'price_unit', 'price_bulk'\n",
    "    ]\n",
    "\n",
    "    if not ingredient_query:\n",
    "        empty = pd.DataFrame(columns=cols)\n",
    "        empty['ingredient'] = []\n",
    "        empty['score_ingredient'] = []\n",
    "        return empty\n",
    "\n",
    "    # Ingredient retrieval should be driven by ingredient evidence, not by full recipe wording.\n",
    "    sem_ingredient = semantic_scores_batch([f'{ingredient_query} mercadona ingrediente'])[0]\n",
    "    lex_scores = lexical_scores_ingredient(ingredient_query)\n",
    "\n",
    "    # Safety: if there is no lexical evidence at all, return empty instead of noisy semantic matches.\n",
    "    if float(lex_scores.max()) <= 0.0:\n",
    "        empty = pd.DataFrame(columns=cols)\n",
    "        empty['ingredient'] = []\n",
    "        empty['score_ingredient'] = []\n",
    "        return empty\n",
    "\n",
    "    # Optional weak context signal to disambiguate when several ingredient candidates are valid.\n",
    "    if context_query:\n",
    "        sem_context = semantic_scores_batch([context_query])[0]\n",
    "    else:\n",
    "        sem_context = np.zeros(N_DOCS, dtype=np.float32)\n",
    "\n",
    "    base_scores = alpha * sem_ingredient + (1.0 - alpha) * lex_scores\n",
    "    hybrid_scores = 0.90 * base_scores + 0.10 * sem_context\n",
    "\n",
    "    any_mask, all_mask = _ingredient_token_masks(ingredient_query)\n",
    "    if any_mask.any():\n",
    "        hybrid_scores = np.where(any_mask, hybrid_scores + 0.25, hybrid_scores * 0.10)\n",
    "    if all_mask.any():\n",
    "        hybrid_scores = np.where(all_mask, hybrid_scores + 0.15, hybrid_scores)\n",
    "\n",
    "    candidate_n = min(N_DOCS, max(top_n * 8, top_n))\n",
    "    top_idx = np.argsort(-hybrid_scores)[:candidate_n]\n",
    "\n",
    "    out_cols = cols + ['ingredientes']\n",
    "    out = chunks.iloc[top_idx][out_cols].copy()\n",
    "    out['ingredient'] = ingredient\n",
    "    out['score_ingredient'] = hybrid_scores[top_idx]\n",
    "\n",
    "    out['category_text'] = (\n",
    "        out['category'].fillna('').astype(str).map(normalize_text)\n",
    "        + ' '\n",
    "        + out['subcategory'].fillna('').astype(str).map(normalize_text)\n",
    "    )\n",
    "\n",
    "    non_food_regex = re.compile(\n",
    "        r'\\b(maquill|cuidado|perfume|colonia|higiene|limpieza|hogar|bebe|mascota|'\n",
    "        r'parafarm|deterg|desodor|capilar|depil|afeitad|insectic|lavavaj|suavizante|'\n",
    "        r'papel|celulosa|escoba|fregona|ambientador)\\b'\n",
    "    )\n",
    "\n",
    "    non_food_mask = out['category_text'].apply(lambda txt: bool(non_food_regex.search(txt)))\n",
    "    if (~non_food_mask).any():\n",
    "        out = out[~non_food_mask].copy()\n",
    "\n",
    "    ing_tokens = {\n",
    "        t for t in tokenize(ingredient_query)\n",
    "        if len(t) > 2 and t not in STOPWORDS\n",
    "    }\n",
    "\n",
    "    descriptor_tokens = STOPWORDS | {\n",
    "        'hacendado', 'mercadona', 'valor', 'marca', 'extra', 'ultra', 'ultracongelado',\n",
    "        'congelado', 'fresco', 'fresca', 'fino', 'fina', 'grueso', 'gruesa', 'mesa',\n",
    "        'natural', 'clasico', 'clasica', 'entero', 'entera', 'semidesnatada', 'desnatada',\n",
    "        'kg', 'g', 'gr', 'ml', 'l', 'litro', 'litros', 'ud', 'u', 'pack', 'paquete',\n",
    "        'bote', 'botella', 'lata', 'tarro', 'bolsa'\n",
    "    }\n",
    "\n",
    "    def _candidate_penalty(row):\n",
    "        if not ing_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        product_name = normalize_text(row.get('product_name'))\n",
    "        name_tokens = {\n",
    "            t for t in tokenize(product_name)\n",
    "            if len(t) > 2 and t not in STOPWORDS\n",
    "        }\n",
    "\n",
    "        ingredientes_text = normalize_text(row.get('ingredientes'))\n",
    "        ingredient_list_tokens = {\n",
    "            t for t in tokenize(ingredientes_text)\n",
    "            if len(t) > 2 and t not in STOPWORDS\n",
    "        }\n",
    "\n",
    "        category_text = normalize_text(row.get('category')) + ' ' + normalize_text(row.get('subcategory'))\n",
    "\n",
    "        penalty = 0.0\n",
    "\n",
    "        coverage = len(ing_tokens & name_tokens) / max(1, len(ing_tokens))\n",
    "        if coverage < 1.0:\n",
    "            penalty += (1.0 - coverage) * 0.45\n",
    "\n",
    "        # Penalize strongly processed/composite naming patterns for base ingredients.\n",
    "        if ',' in product_name:\n",
    "            penalty += 0.12\n",
    "        if ' y ' in f' {product_name} ':\n",
    "            penalty += 0.10\n",
    "        if re.search(r'\\b(sabor|rellen|mix|fusion|cocktail|preparad|listo)\\b', product_name):\n",
    "            penalty += 0.20\n",
    "\n",
    "        # For short ingredient names, aggressively penalize unrelated qualifiers.\n",
    "        foreign_name_tokens = [\n",
    "            t for t in name_tokens\n",
    "            if t not in ing_tokens and t not in descriptor_tokens\n",
    "        ]\n",
    "        if len(ing_tokens) <= 1:\n",
    "            penalty += min(0.70, 0.32 * len(foreign_name_tokens))\n",
    "        else:\n",
    "            penalty += min(0.20, 0.05 * len(foreign_name_tokens))\n",
    "\n",
    "        if ingredient_list_tokens:\n",
    "            foreign_ingredient_tokens = [\n",
    "                t for t in ingredient_list_tokens\n",
    "                if t not in ing_tokens and t not in descriptor_tokens\n",
    "            ]\n",
    "            if len(ing_tokens) <= 1:\n",
    "                penalty += min(0.45, 0.015 * len(foreign_ingredient_tokens))\n",
    "            else:\n",
    "                penalty += min(0.22, 0.010 * len(foreign_ingredient_tokens))\n",
    "\n",
    "        # Generic ingredient disambiguation rules (not recipe-specific):\n",
    "        # - plain egg should avoid quail/claras/cooked variants\n",
    "        if ('huevo' in ing_tokens or 'huevos' in ing_tokens) and re.search(r'\\b(codorniz|claras?|cocid[oa]s?)\\b', product_name):\n",
    "            penalty += 0.40\n",
    "\n",
    "        # - dairy cream intent should avoid vegetable/soup creams\n",
    "        dairy_cream_intent = (\n",
    "            ('crema' in ing_tokens and 'leche' in ing_tokens)\n",
    "            or ('nata' in ing_tokens)\n",
    "        )\n",
    "        if dairy_cream_intent:\n",
    "            if re.search(r'\\b(gazpacho|sopa|cremas?)\\b', category_text):\n",
    "                penalty += 1.20\n",
    "            if re.search(r'\\b(calabaza|calabacin|verduras?|pollo|setas?)\\b', product_name):\n",
    "                penalty += 1.20\n",
    "\n",
    "        return float(penalty)\n",
    "\n",
    "    out['candidate_penalty'] = out.apply(_candidate_penalty, axis=1)\n",
    "    out['score_ingredient'] = out['score_ingredient'] - out['candidate_penalty']\n",
    "\n",
    "    # Deduplicate same product across duplicated category branches.\n",
    "    out = out.sort_values('score_ingredient', ascending=False).drop_duplicates(subset=['product_id'], keep='first')\n",
    "\n",
    "    final_cols = cols + ['ingredient', 'score_ingredient']\n",
    "    return out[final_cols].head(top_n).reset_index(drop=True)\n",
    "\n",
    "def tool_get_products_for_ingredients(ingredients, per_ingredient=10, alpha=0.35, recipe_query=None):\n",
    "    \"\"\"\n",
    "    Tool-like helper: for each ingredient, returns a ranked product shortlist.\n",
    "    \"\"\"\n",
    "    catalog = {}\n",
    "    for ingredient in ingredients:\n",
    "        hits = retrieve_products_for_ingredient(ingredient, top_n=per_ingredient, alpha=alpha, recipe_query=recipe_query)\n",
    "        catalog[ingredient] = hits\n",
    "    return catalog\n",
    "\n",
    "def _choose_best_candidate(df_hits, required_qty, required_unit, requirement_source='fallback'):\n",
    "    if df_hits is None or df_hits.empty:\n",
    "        return None\n",
    "\n",
    "    req_dim, req_base = _to_dim_base(required_qty, required_unit)\n",
    "    source = normalize_text(requirement_source)\n",
    "    consider_qty = (source != 'fallback') and (req_dim is not None) and (req_base is not None) and (req_base > 0)\n",
    "\n",
    "    ranked = []\n",
    "\n",
    "    for _, row in df_hits.iterrows():\n",
    "        price = _safe_float(row.get('price_unit'))\n",
    "        if price is None:\n",
    "            continue\n",
    "\n",
    "        score = _safe_float(row.get('score_ingredient'))\n",
    "        if score is None:\n",
    "            score = 0.0\n",
    "\n",
    "        unit_size = _safe_float(row.get('unit_size'))\n",
    "        size_fmt = row.get('size_format')\n",
    "        pack_dim, pack_base = _to_dim_base(unit_size, size_fmt)\n",
    "\n",
    "        comparable = consider_qty and (pack_dim == req_dim) and (pack_base is not None) and (pack_base > 0)\n",
    "        if comparable:\n",
    "            units_to_buy = max(1, math.ceil(req_base / pack_base))\n",
    "            purchase_cost = units_to_buy * price\n",
    "        else:\n",
    "            units_to_buy = 1\n",
    "            purchase_cost = price\n",
    "\n",
    "        ranked.append({\n",
    "            'row': row,\n",
    "            'score': float(score),\n",
    "            'comparable': bool(comparable),\n",
    "            'units_to_buy': int(units_to_buy),\n",
    "            'purchase_cost': float(purchase_cost),\n",
    "        })\n",
    "\n",
    "    if not ranked:\n",
    "        return df_hits.iloc[0]\n",
    "\n",
    "    if consider_qty:\n",
    "        comparable_rows = [x for x in ranked if x['comparable']]\n",
    "        if comparable_rows:\n",
    "            comparable_rows.sort(key=lambda x: (x['purchase_cost'], x['units_to_buy'], -x['score']))\n",
    "            return comparable_rows[0]['row']\n",
    "\n",
    "    # Fallback: keep relevance first, then prefer cheaper package.\n",
    "    ranked.sort(key=lambda x: (-x['score'], x['purchase_cost']))\n",
    "    return ranked[0]['row']\n",
    "\n",
    "def _compute_buy_and_escandallo(chosen_row, required_qty, required_unit):\n",
    "    if chosen_row is None:\n",
    "        return {\n",
    "            'units_to_buy': None,\n",
    "            'purchase_cost_eur': None,\n",
    "            'escandallo_cost_eur': None,\n",
    "            'notes': 'Sin producto candidato',\n",
    "        }\n",
    "\n",
    "    price_unit = _safe_float(chosen_row.get('price_unit'))\n",
    "    unit_size = _safe_float(chosen_row.get('unit_size'))\n",
    "    size_format = chosen_row.get('size_format')\n",
    "\n",
    "    req_dim, req_base = _to_dim_base(required_qty, required_unit)\n",
    "    pack_dim, pack_base = _to_dim_base(unit_size, size_format)\n",
    "\n",
    "    if price_unit is None:\n",
    "        return {\n",
    "            'units_to_buy': None,\n",
    "            'purchase_cost_eur': None,\n",
    "            'escandallo_cost_eur': None,\n",
    "            'notes': 'Producto sin price_unit',\n",
    "        }\n",
    "\n",
    "    # Regla clave: compra siempre envase completo (redondeo al alza)\n",
    "    if req_dim is not None and pack_dim == req_dim and pack_base is not None and pack_base > 0 and req_base is not None:\n",
    "        units_to_buy = max(1, math.ceil(req_base / pack_base))\n",
    "        purchase_cost = units_to_buy * price_unit\n",
    "        escandallo_cost = (req_base / pack_base) * price_unit\n",
    "        return {\n",
    "            'units_to_buy': units_to_buy,\n",
    "            'purchase_cost_eur': purchase_cost,\n",
    "            'escandallo_cost_eur': escandallo_cost,\n",
    "            'notes': 'OK unidades comparables',\n",
    "        }\n",
    "\n",
    "    # Fallback: no se puede convertir unidades -> compra minima 1 unidad.\n",
    "    return {\n",
    "        'units_to_buy': 1,\n",
    "        'purchase_cost_eur': price_unit,\n",
    "        'escandallo_cost_eur': None,\n",
    "        'notes': 'Unidad no comparable, compra minima 1 unidad',\n",
    "    }\n",
    "\n",
    "def _dedupe_plan_rows_by_product(plan_df):\n",
    "    if plan_df is None or plan_df.empty or 'product_id' not in plan_df.columns:\n",
    "        return plan_df\n",
    "\n",
    "    df = plan_df.copy()\n",
    "    df['_row_order'] = np.arange(len(df), dtype=int)\n",
    "\n",
    "    with_product = df[df['product_id'].notna()].copy()\n",
    "    without_product = df[df['product_id'].isna()].copy()\n",
    "\n",
    "    if with_product.empty:\n",
    "        return df.drop(columns=['_row_order'], errors='ignore').reset_index(drop=True)\n",
    "\n",
    "    with_product['_required_qty_num'] = with_product['required_qty'].apply(_safe_float)\n",
    "    with_product['_required_qty_num'] = with_product['_required_qty_num'].fillna(-1.0)\n",
    "\n",
    "    with_product = with_product.sort_values(\n",
    "        by=['product_id', '_required_qty_num', '_row_order'],\n",
    "        ascending=[True, False, True],\n",
    "    ).drop_duplicates(subset=['product_id'], keep='first')\n",
    "\n",
    "    collapsed = pd.concat([with_product, without_product], ignore_index=True, sort=False)\n",
    "    collapsed = collapsed.sort_values('_row_order').drop(\n",
    "        columns=['_row_order', '_required_qty_num'],\n",
    "        errors='ignore',\n",
    "    )\n",
    "\n",
    "    return collapsed.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_recipe_cost_plan(ingredient_catalog, ingredients, query):\n",
    "    servings = parse_servings(query, default=4)\n",
    "    rows = []\n",
    "\n",
    "    for ingredient in ingredients:\n",
    "        required_qty, required_unit, req_source = estimate_required_quantity(ingredient, servings=servings)\n",
    "        hits = ingredient_catalog.get(ingredient)\n",
    "        chosen = _choose_best_candidate(hits, required_qty, required_unit, req_source)\n",
    "\n",
    "        if chosen is None:\n",
    "            rows.append({\n",
    "                'ingredient': ingredient,\n",
    "                'required_qty': required_qty,\n",
    "                'required_unit': required_unit,\n",
    "                'requirement_source': req_source,\n",
    "                'product_id': None,\n",
    "                'product_name': None,\n",
    "                'unit_size': None,\n",
    "                'size_format': None,\n",
    "                'price_unit': None,\n",
    "                'units_to_buy': None,\n",
    "                'purchase_cost_eur': None,\n",
    "                'escandallo_cost_eur': None,\n",
    "                'notes': 'Sin candidatos',\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        cost_info = _compute_buy_and_escandallo(chosen, required_qty, required_unit)\n",
    "        rows.append({\n",
    "            'ingredient': ingredient,\n",
    "            'required_qty': required_qty,\n",
    "            'required_unit': required_unit,\n",
    "            'requirement_source': req_source,\n",
    "            'product_id': chosen.get('product_id'),\n",
    "            'product_name': chosen.get('product_name'),\n",
    "            'unit_size': _safe_float(chosen.get('unit_size')),\n",
    "            'size_format': chosen.get('size_format'),\n",
    "            'price_unit': _safe_float(chosen.get('price_unit')),\n",
    "            'units_to_buy': cost_info['units_to_buy'],\n",
    "            'purchase_cost_eur': cost_info['purchase_cost_eur'],\n",
    "            'escandallo_cost_eur': cost_info['escandallo_cost_eur'],\n",
    "            'notes': cost_info['notes'],\n",
    "        })\n",
    "\n",
    "    plan_df = pd.DataFrame(rows)\n",
    "    plan_df = _dedupe_plan_rows_by_product(plan_df)\n",
    "\n",
    "    total_purchase = _safe_float(plan_df['purchase_cost_eur'].dropna().sum()) if not plan_df.empty else 0.0\n",
    "    total_escandallo = _safe_float(plan_df['escandallo_cost_eur'].dropna().sum()) if not plan_df.empty else 0.0\n",
    "\n",
    "    missing_purchase = plan_df[plan_df['purchase_cost_eur'].isna()]['ingredient'].tolist() if not plan_df.empty else []\n",
    "    missing_esc = plan_df[plan_df['escandallo_cost_eur'].isna()]['ingredient'].tolist() if not plan_df.empty else []\n",
    "\n",
    "    summary = {\n",
    "        'servings': servings,\n",
    "        'total_purchase_eur': total_purchase,\n",
    "        'total_escandallo_eur': total_escandallo,\n",
    "        'missing_purchase_ingredients': missing_purchase,\n",
    "        'missing_escandallo_ingredients': missing_esc,\n",
    "    }\n",
    "\n",
    "    return plan_df, summary\n",
    "\n",
    "def format_ingredient_catalog_text(catalog, max_items=6):\n",
    "    blocks = []\n",
    "    for ingredient, df_hits in catalog.items():\n",
    "        lines = [f'Ingrediente: {ingredient}']\n",
    "        if df_hits.empty:\n",
    "            lines.append('- Sin candidatos en el dataset')\n",
    "        else:\n",
    "            for _, row in df_hits.head(max_items).iterrows():\n",
    "                lines.append(\n",
    "                    '- '\n",
    "                    f\"product_id={row['product_id']}; \"\n",
    "                    f\"nombre={row['product_name']}; \"\n",
    "                    f\"categoria={row['category']}; \"\n",
    "                    f\"precio_unit={row['price_unit']}; \"\n",
    "                    f\"formato={row['unit_size']} {row['size_format']}; \"\n",
    "                    f\"score={row['score_ingredient']:.4f}\"\n",
    "                )\n",
    "        blocks.append('\\n'.join(lines))\n",
    "    return '\\n\\n'.join(blocks)\n",
    "\n",
    "def format_cost_plan_text(plan_df, summary):\n",
    "    if plan_df is None or plan_df.empty:\n",
    "        return 'Sin plan de costes.'\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"Personas: {summary['servings']}\")\n",
    "    lines.append(f\"TOTAL_COMPRA_EUR (envase completo): {_fmt_money(summary['total_purchase_eur'])}\")\n",
    "    lines.append(f\"TOTAL_ESCANDALLO_EUR (consumo real): {_fmt_money(summary['total_escandallo_eur'])}\")\n",
    "\n",
    "    if summary['missing_purchase_ingredients']:\n",
    "        lines.append('Faltan precios de compra para: ' + ', '.join(summary['missing_purchase_ingredients']))\n",
    "    if summary['missing_escandallo_ingredients']:\n",
    "        lines.append('No se pudo calcular escandallo para: ' + ', '.join(summary['missing_escandallo_ingredients']))\n",
    "\n",
    "    lines.append('')\n",
    "    lines.append('| ingrediente | product_id | producto | req_qty | pack_size | price_unit | units_to_buy | purchase_cost_eur | escandallo_cost_eur |')\n",
    "    lines.append('|---|---:|---|---:|---|---:|---:|---:|---:|')\n",
    "\n",
    "    for _, r in plan_df.iterrows():\n",
    "        lines.append(\n",
    "            f\"| {r.get('ingredient')} | {r.get('product_id')} | {r.get('product_name')} | \"\n",
    "            f\"{_fmt_num(r.get('required_qty'), 3)} {r.get('required_unit')} | \"\n",
    "            f\"{_fmt_num(r.get('unit_size'), 3)} {r.get('size_format')} | \"\n",
    "            f\"{_fmt_money(r.get('price_unit'))} | {r.get('units_to_buy')} | \"\n",
    "            f\"{_fmt_money(r.get('purchase_cost_eur'))} | {_fmt_money(r.get('escandallo_cost_eur'))} |\"\n",
    "        )\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def _fmt_money_es(v):\n",
    "    n = _safe_float(v)\n",
    "    if n is None:\n",
    "        return 'N/D'\n",
    "    return f\"{n:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n",
    "\n",
    "def _fmt_recipe_quantity_number(v, decimals=2):\n",
    "    n = _safe_float(v)\n",
    "    if n is None:\n",
    "        return 'N/D'\n",
    "\n",
    "    rounded = round(n)\n",
    "    if abs(n - rounded) < 1e-9:\n",
    "        return str(int(rounded))\n",
    "\n",
    "    return f\"{n:.{decimals}f}\".rstrip('0').rstrip('.')\n",
    "\n",
    "\n",
    "def _format_qty_for_recipe_line(ingredient, required_qty, required_unit):\n",
    "    qty = _safe_float(required_qty)\n",
    "    unit = normalize_text(required_unit)\n",
    "    ing = normalize_text(ingredient)\n",
    "\n",
    "    if qty is None:\n",
    "        return 'cantidad N/D'\n",
    "\n",
    "    condiments = ('sal', 'pimienta', 'azafran', 'pimenton', 'laurel')\n",
    "\n",
    "    if unit == 'kg':\n",
    "        grams = qty * 1000.0\n",
    "        if any(c in ing for c in condiments):\n",
    "            if grams <= 6:\n",
    "                return '1 cucharadita'\n",
    "            if grams <= 20:\n",
    "                return '1 cucharada'\n",
    "        if grams < 1000:\n",
    "            return f\"{int(round(grams))} g\"\n",
    "        return f\"{_fmt_recipe_quantity_number(qty, 2)} kg\"\n",
    "\n",
    "    if unit == 'l':\n",
    "        ml = qty * 1000.0\n",
    "        if 'limon' in ing and ml <= 60:\n",
    "            return '1 chorro'\n",
    "        if 'aceite' in ing and ml <= 80:\n",
    "            return '1 chorro'\n",
    "        if ml < 1000:\n",
    "            return f\"{int(round(ml))} ml\"\n",
    "        return f\"{_fmt_recipe_quantity_number(qty, 2)} L\"\n",
    "\n",
    "    if unit == 'ud':\n",
    "        if 'limon' in ing:\n",
    "            return '1 chorro'\n",
    "        n = max(1, int(round(qty)))\n",
    "        return f\"{n} ud\"\n",
    "\n",
    "    unit_raw = str(required_unit).strip() if required_unit is not None else ''\n",
    "    if unit_raw:\n",
    "        return f\"{_fmt_recipe_quantity_number(qty, 2)} {unit_raw}\"\n",
    "    return _fmt_recipe_quantity_number(qty, 2)\n",
    "\n",
    "def build_block1_ingredients_mercadona(plan_df):\n",
    "    if plan_df is None or plan_df.empty:\n",
    "        return 'No se encontraron ingredientes/productos en Mercadona para esta receta.'\n",
    "\n",
    "    lines = []\n",
    "    seen_products = set()\n",
    "\n",
    "    for _, r in plan_df.iterrows():\n",
    "        ingredient = r.get('ingredient')\n",
    "        product_name = r.get('product_name')\n",
    "        req_qty = r.get('required_qty')\n",
    "        req_unit = r.get('required_unit')\n",
    "        product_id = r.get('product_id')\n",
    "        purchase_cost = _safe_float(r.get('purchase_cost_eur'))\n",
    "        units_to_buy = _safe_float(r.get('units_to_buy'))\n",
    "\n",
    "        if product_name is None or str(product_name) == 'nan':\n",
    "            lines.append(f\"- {ingredient}: sin producto identificado en el dataset\")\n",
    "            continue\n",
    "\n",
    "        if product_id is None or str(product_id) == 'nan':\n",
    "            product_key = f\"name:{normalize_text(product_name)}\"\n",
    "        else:\n",
    "            product_key = f\"id:{str(product_id).strip()}\"\n",
    "\n",
    "        if product_key in seen_products:\n",
    "            continue\n",
    "\n",
    "        seen_products.add(product_key)\n",
    "\n",
    "        qty_text = _format_qty_for_recipe_line(ingredient, req_qty, req_unit)\n",
    "\n",
    "        if purchase_cost is None:\n",
    "            price_text = 'N/D'\n",
    "        else:\n",
    "            price_text = f\"{_fmt_money_es(purchase_cost)}€\"\n",
    "\n",
    "        if units_to_buy is not None and units_to_buy > 1:\n",
    "            units_text = f\" (x{int(round(units_to_buy))} envases)\"\n",
    "        else:\n",
    "            units_text = ''\n",
    "\n",
    "        lines.append(f\"- {qty_text} {product_name} ({price_text}){units_text}\")\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def tool_get_total_purchase_price(cost_summary):\n",
    "    return ''\n",
    "\n",
    "def _build_recipe_prompt_context(plan_df, max_items=14):\n",
    "    if plan_df is None or plan_df.empty:\n",
    "        return 'No hay productos concretos disponibles.'\n",
    "\n",
    "    lines = []\n",
    "    for _, r in plan_df.head(max_items).iterrows():\n",
    "        product_name = r.get('product_name')\n",
    "        ingredient = r.get('ingredient')\n",
    "        req_qty = _fmt_num(r.get('required_qty'), 3)\n",
    "        req_unit = r.get('required_unit')\n",
    "        if product_name is None or str(product_name) == 'nan':\n",
    "            continue\n",
    "        lines.append(f\"- {ingredient}: {product_name} ({req_qty} {req_unit})\")\n",
    "\n",
    "    return '\\n'.join(lines) if lines else 'No hay productos concretos disponibles.'\n",
    "\n",
    "def build_block3_recipe_text(query, plan_df, model=CHAT_MODEL, max_chars=1000):\n",
    "    context_lines = _build_recipe_prompt_context(plan_df)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Escribe una receta breve en español para esta solicitud: {query}.\\n\"\n",
    "        f\"Usa de referencia estos ingredientes/productos:\\n{context_lines}\\n\\n\"\n",
    "        f\"Reglas estrictas:\\n\"\n",
    "        f\"- Máximo {max_chars} caracteres.\\n\"\n",
    "        f\"- Texto corrido con pasos claros (sin tablas).\\n\"\n",
    "        f\"- No inventes precios.\\n\"\n",
    "        f\"- Devuelve solo el texto de la receta.\"\n",
    "    )\n",
    "\n",
    "    raw_resp = None\n",
    "    text = ''\n",
    "    try:\n",
    "        raw_resp = client.responses.create(model=model, input=prompt)\n",
    "        text = (raw_resp.output_text or '').strip()\n",
    "    except Exception as exc:\n",
    "        text = f\"No se pudo generar el texto de receta automáticamente ({exc}).\"\n",
    "\n",
    "    if len(text) > max_chars:\n",
    "        text = text[: max_chars - 1].rstrip() + '…'\n",
    "\n",
    "    return text, raw_resp\n",
    "\n",
    "def compose_structured_answer(block1, block2, block3):\n",
    "    parts = [\n",
    "        'Ingredientes de Mercadona:',\n",
    "        str(block1).strip(),\n",
    "    ]\n",
    "\n",
    "    block2_text = str(block2).strip() if block2 is not None else ''\n",
    "    if block2_text:\n",
    "        parts.append(block2_text)\n",
    "\n",
    "    parts.extend([\n",
    "        'Receta y pasos:',\n",
    "        str(block3).strip(),\n",
    "    ])\n",
    "\n",
    "    return '\\n\\n'.join(parts)\n",
    "\n",
    "def _catalog_preview(catalog, n=3):\n",
    "    preview = {}\n",
    "    for ing, df_hits in catalog.items():\n",
    "        preview[ing] = [\n",
    "            {\n",
    "                'product_id': r['product_id'],\n",
    "                'product_name': r['product_name'],\n",
    "                'price_unit': r['price_unit'],\n",
    "            }\n",
    "            for _, r in df_hits.head(n).iterrows()\n",
    "        ]\n",
    "    return preview\n",
    "\n",
    "def ask_agent(\n",
    "    query,\n",
    "    top_k=20,\n",
    "    model=CHAT_MODEL,\n",
    "    retrieval_mode='hybrid',\n",
    "    alpha=0.65,\n",
    "    recipe_mode='auto',\n",
    "    use_ingredient_tool=True,\n",
    "    candidates_per_ingredient=10,\n",
    "):\n",
    "    hits, subqueries, inferred_ingredients = retrieve(\n",
    "        query,\n",
    "        top_k=top_k,\n",
    "        mode=retrieval_mode,\n",
    "        alpha=alpha,\n",
    "        recipe_mode=recipe_mode,\n",
    "    )\n",
    "\n",
    "    ingredient_catalog = {}\n",
    "    ingredient_catalog_text = ''\n",
    "    cost_plan_df = pd.DataFrame()\n",
    "    cost_summary = {\n",
    "        'servings': parse_servings(query, default=4),\n",
    "        'total_purchase_eur': 0.0,\n",
    "        'total_escandallo_eur': 0.0,\n",
    "        'missing_purchase_ingredients': [],\n",
    "        'missing_escandallo_ingredients': [],\n",
    "    }\n",
    "    cost_plan_text = 'Sin plan de costes.'\n",
    "\n",
    "    if use_ingredient_tool and inferred_ingredients:\n",
    "        ingredient_catalog = tool_get_products_for_ingredients(\n",
    "            inferred_ingredients,\n",
    "            per_ingredient=candidates_per_ingredient,\n",
    "            alpha=0.35,\n",
    "            recipe_query=query,\n",
    "        )\n",
    "        ingredient_catalog_text = format_ingredient_catalog_text(ingredient_catalog, max_items=6)\n",
    "\n",
    "        cost_plan_df, cost_summary = build_recipe_cost_plan(\n",
    "            ingredient_catalog=ingredient_catalog,\n",
    "            ingredients=inferred_ingredients,\n",
    "            query=query,\n",
    "        )\n",
    "        cost_plan_text = format_cost_plan_text(cost_plan_df, cost_summary)\n",
    "\n",
    "    block_1 = build_block1_ingredients_mercadona(cost_plan_df)\n",
    "    block_2 = tool_get_total_purchase_price(cost_summary)\n",
    "    block_3, raw_resp_block3 = build_block3_recipe_text(query, cost_plan_df, model=model, max_chars=1000)\n",
    "    structured_answer = compose_structured_answer(block_1, block_2, block_3)\n",
    "\n",
    "    hit_cols = [\n",
    "        'product_id', 'product_name', 'category', 'price_unit',\n",
    "        'unit_size', 'size_format', 'score'\n",
    "    ]\n",
    "    for extra in ('score_semantic', 'score_lexical'):\n",
    "        if extra in hits.columns:\n",
    "            hit_cols.append(extra)\n",
    "\n",
    "    return {\n",
    "        'answer': structured_answer,\n",
    "        'block_1': block_1,\n",
    "        'block_2': block_2,\n",
    "        'block_3': block_3,\n",
    "        'hits': hits[hit_cols],\n",
    "        'subqueries': subqueries,\n",
    "        'inferred_ingredients': inferred_ingredients,\n",
    "        'ingredient_catalog': ingredient_catalog,\n",
    "        'ingredient_catalog_preview': _catalog_preview(ingredient_catalog, n=3),\n",
    "        'cost_plan': cost_plan_df,\n",
    "        'cost_summary': cost_summary,\n",
    "        'cost_plan_text': cost_plan_text,\n",
    "        'ingredient_catalog_text': ingredient_catalog_text,\n",
    "        'raw_response': raw_resp_block3,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f163e664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredientes de Mercadona:\n",
      "- 1 ud Muslos de pollo deshuesados con piel\n",
      "- 1 chorro Aceite de oliva 1º Hacendado\n",
      "- 20 g Picada de ajo y perejil\n",
      "- 150 g Salteado de pimientos y cebolla cortado y lavado\n",
      "- 150 g Pimiento verde freír\n",
      "- 200 g Tomate rallado\n",
      "- 150 g Zanahorias\n",
      "- 1 ud Judías verdes extrafinas Hacendado\n",
      "- 1 ud Guisantes extra Hacendado\n",
      "- 1 ud Calabacín verde\n",
      "- 1 ud Patatas para microondas\n",
      "- 1 cucharada Bicarbonato de sódio Hacendado\n",
      "\n",
      "Precio de los productos a comprar para esta receta: 36,78€\n",
      "\n",
      "Receta y pasos:\n",
      "Calienta 0,060 l de aceite de oliva en una sartén grande y sofríe 0,020 kg de ajo y perejil picados junto con 0,150 kg de cebolla y pimiento verde hasta que estén tiernos. Añade 1 kg de muslos de pollo deshuesados con piel y dóralos por ambos lados. Incorpora 0,200 kg de tomate rallado y cocina a fuego medio. Mientras tanto, corta 0,150 kg de zanahorias, 1 kg de calabacín y 1 kg de patatas para microondas en trozos medianos. Añade también 1 kg de judías verdes y 1 kg de guisantes. Agrega las verduras a la sartén con el pollo, salpimienta con 0,010 kg de bicarbonato sódico (sirve para realzar el sabor) y mezcla bien. Cubre y cocina a fuego lento hasta que el pollo y las verduras estén tiernos, aproximadamente 30 minutos. Sirve caliente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredient</th>\n",
       "      <th>required_qty</th>\n",
       "      <th>required_unit</th>\n",
       "      <th>requirement_source</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>unit_size</th>\n",
       "      <th>size_format</th>\n",
       "      <th>price_unit</th>\n",
       "      <th>units_to_buy</th>\n",
       "      <th>purchase_cost_eur</th>\n",
       "      <th>escandallo_cost_eur</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>muslos de pollo</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ud</td>\n",
       "      <td>fallback</td>\n",
       "      <td>2788.0</td>\n",
       "      <td>Muslos de pollo deshuesados con piel</td>\n",
       "      <td>0.50</td>\n",
       "      <td>kg</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unidad no comparable, compra minima 1 unidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aceite de oliva</td>\n",
       "      <td>0.06</td>\n",
       "      <td>l</td>\n",
       "      <td>aceite de oliva</td>\n",
       "      <td>4641.0</td>\n",
       "      <td>Aceite de oliva 1º Hacendado</td>\n",
       "      <td>5.00</td>\n",
       "      <td>l</td>\n",
       "      <td>20.85</td>\n",
       "      <td>1</td>\n",
       "      <td>20.85</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ajo</td>\n",
       "      <td>0.02</td>\n",
       "      <td>kg</td>\n",
       "      <td>ajo</td>\n",
       "      <td>40475.0</td>\n",
       "      <td>Picada de ajo y perejil</td>\n",
       "      <td>0.10</td>\n",
       "      <td>kg</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cebolla</td>\n",
       "      <td>0.15</td>\n",
       "      <td>kg</td>\n",
       "      <td>cebolla</td>\n",
       "      <td>69423.0</td>\n",
       "      <td>Salteado de pimientos y cebolla cortado y lavado</td>\n",
       "      <td>0.40</td>\n",
       "      <td>kg</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pimiento verde</td>\n",
       "      <td>0.15</td>\n",
       "      <td>kg</td>\n",
       "      <td>pimiento</td>\n",
       "      <td>69320.0</td>\n",
       "      <td>Pimiento verde freír</td>\n",
       "      <td>0.10</td>\n",
       "      <td>kg</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tomate</td>\n",
       "      <td>0.20</td>\n",
       "      <td>kg</td>\n",
       "      <td>tomate</td>\n",
       "      <td>69938.0</td>\n",
       "      <td>Tomate rallado</td>\n",
       "      <td>0.29</td>\n",
       "      <td>kg</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zanahoria</td>\n",
       "      <td>0.15</td>\n",
       "      <td>kg</td>\n",
       "      <td>zanahoria</td>\n",
       "      <td>69669.0</td>\n",
       "      <td>Zanahorias</td>\n",
       "      <td>0.50</td>\n",
       "      <td>kg</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>judias verdes</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ud</td>\n",
       "      <td>fallback</td>\n",
       "      <td>20349.0</td>\n",
       "      <td>Judías verdes extrafinas Hacendado</td>\n",
       "      <td>0.53</td>\n",
       "      <td>kg</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unidad no comparable, compra minima 1 unidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>guisantes</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ud</td>\n",
       "      <td>fallback</td>\n",
       "      <td>16416.0</td>\n",
       "      <td>Guisantes extra Hacendado</td>\n",
       "      <td>0.39</td>\n",
       "      <td>kg</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unidad no comparable, compra minima 1 unidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>calabacin</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ud</td>\n",
       "      <td>fallback</td>\n",
       "      <td>69338.0</td>\n",
       "      <td>Calabacín verde</td>\n",
       "      <td>0.43</td>\n",
       "      <td>kg</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unidad no comparable, compra minima 1 unidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>patata</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ud</td>\n",
       "      <td>fallback</td>\n",
       "      <td>69484.0</td>\n",
       "      <td>Patatas para microondas</td>\n",
       "      <td>0.40</td>\n",
       "      <td>kg</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unidad no comparable, compra minima 1 unidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sal</td>\n",
       "      <td>0.01</td>\n",
       "      <td>kg</td>\n",
       "      <td>sal</td>\n",
       "      <td>29006.0</td>\n",
       "      <td>Bicarbonato de sódio Hacendado</td>\n",
       "      <td>0.30</td>\n",
       "      <td>kg</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>OK unidades comparables</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ingredient  required_qty required_unit requirement_source  \\\n",
       "0   muslos de pollo          1.00            ud           fallback   \n",
       "1   aceite de oliva          0.06             l    aceite de oliva   \n",
       "2               ajo          0.02            kg                ajo   \n",
       "3           cebolla          0.15            kg            cebolla   \n",
       "4    pimiento verde          0.15            kg           pimiento   \n",
       "5            tomate          0.20            kg             tomate   \n",
       "6         zanahoria          0.15            kg          zanahoria   \n",
       "7     judias verdes          1.00            ud           fallback   \n",
       "8         guisantes          1.00            ud           fallback   \n",
       "9         calabacin          1.00            ud           fallback   \n",
       "10           patata          1.00            ud           fallback   \n",
       "11              sal          0.01            kg                sal   \n",
       "\n",
       "    product_id                                      product_name  unit_size  \\\n",
       "0       2788.0              Muslos de pollo deshuesados con piel       0.50   \n",
       "1       4641.0                      Aceite de oliva 1º Hacendado       5.00   \n",
       "2      40475.0                           Picada de ajo y perejil       0.10   \n",
       "3      69423.0  Salteado de pimientos y cebolla cortado y lavado       0.40   \n",
       "4      69320.0                              Pimiento verde freír       0.10   \n",
       "5      69938.0                                    Tomate rallado       0.29   \n",
       "6      69669.0                                        Zanahorias       0.50   \n",
       "7      20349.0                Judías verdes extrafinas Hacendado       0.53   \n",
       "8      16416.0                         Guisantes extra Hacendado       0.39   \n",
       "9      69338.0                                   Calabacín verde       0.43   \n",
       "10     69484.0                           Patatas para microondas       0.40   \n",
       "11     29006.0                    Bicarbonato de sódio Hacendado       0.30   \n",
       "\n",
       "   size_format  price_unit  units_to_buy  purchase_cost_eur  \\\n",
       "0           kg        2.78             1               2.78   \n",
       "1            l       20.85             1              20.85   \n",
       "2           kg        1.50             1               1.50   \n",
       "3           kg        2.40             1               2.40   \n",
       "4           kg        0.25             2               0.50   \n",
       "5           kg        1.45             1               1.45   \n",
       "6           kg        0.80             1               0.80   \n",
       "7           kg        1.90             1               1.90   \n",
       "8           kg        0.95             1               0.95   \n",
       "9           kg        1.05             1               1.05   \n",
       "10          kg        1.50             1               1.50   \n",
       "11          kg        1.10             1               1.10   \n",
       "\n",
       "    escandallo_cost_eur                                         notes  \n",
       "0                   NaN  Unidad no comparable, compra minima 1 unidad  \n",
       "1              0.250200                       OK unidades comparables  \n",
       "2              0.300000                       OK unidades comparables  \n",
       "3              0.900000                       OK unidades comparables  \n",
       "4              0.375000                       OK unidades comparables  \n",
       "5              1.000000                       OK unidades comparables  \n",
       "6              0.240000                       OK unidades comparables  \n",
       "7                   NaN  Unidad no comparable, compra minima 1 unidad  \n",
       "8                   NaN  Unidad no comparable, compra minima 1 unidad  \n",
       "9                   NaN  Unidad no comparable, compra minima 1 unidad  \n",
       "10                  NaN  Unidad no comparable, compra minima 1 unidad  \n",
       "11             0.036667                       OK unidades comparables  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'Dame la receta de muslos de pollo con menestra de verduras'\n",
    "result = ask_agent(\n",
    "    question,\n",
    "    top_k=35,\n",
    "    retrieval_mode='hybrid',\n",
    "    alpha=0.65,\n",
    "    recipe_mode='auto',\n",
    "    use_ingredient_tool=True,\n",
    "    candidates_per_ingredient=12,\n",
    ")\n",
    "print(result['answer'])\n",
    "result['cost_plan'].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01748d2d",
   "metadata": {},
   "source": [
    "## Notas\n",
    "- Si cambias el Excel, vuelve a ejecutar la celda de `chunks` y luego `ensure_embeddings(..., force_rebuild=True)`.\n",
    "- Puedes bajar coste usando menos `top_k` o cambiando a un modelo chat mas pequeno.\n",
    "- Puedes guardar respuestas historicas en CSV si quieres trazabilidad de preguntas/respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0b7787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-11 23:17:35,951] INFO Inicializando runtime desde notebook: /home/wencm/RecetONA/mercadona_rag_notebook.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel: /home/wencm/RecetONA/mercadona_data.xlsx\n",
      "Cache: /home/wencm/RecetONA/rag_cache\n",
      "Python: /home/wencm/WhaRAGBot/.venv/bin/python\n",
      "OPENAI_API_KEY presente: True\n",
      "Filas: 4553\n",
      "Chunks guardados en: /home/wencm/RecetONA/rag_cache/chunks.csv\n",
      "Embeddings cargados desde cache: /home/wencm/RecetONA/rag_cache/embeddings.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-11 23:17:44,166] INFO API local escuchando en http://127.0.0.1:8787\n",
      "[2026-02-11 23:17:44,166] INFO Endpoints: GET /health, POST /chat\n",
      "[2026-02-11 23:17:44,450] INFO 127.0.0.1 - \"GET /health HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving HTTP on 127.0.0.1 port 8080 (http://127.0.0.1:8080/) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Feb/2026 23:17:44] \"GET /main.html HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API RAG local iniciada en: http://127.0.0.1:8787\n",
      "Frontend local iniciado en: http://127.0.0.1:8080/main.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Feb/2026 23:17:49] \"GET /main.html HTTP/1.1\" 200 -\n",
      "[2026-02-11 23:17:49,936] INFO 127.0.0.1 - \"GET /health HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Feb/2026 23:17:50] code 404, message File not found\n",
      "127.0.0.1 - - [11/Feb/2026 23:17:50] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:8080/main.html\" target=\"_blank\">Abrir chat RecetONA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat abierto en el navegador.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-11 23:18:05,920] INFO 127.0.0.1 - \"OPTIONS /chat HTTP/1.1\" 200 -\n",
      "[2026-02-11 23:18:08,587] INFO HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:09,178] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:09,412] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:09,620] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:09,778] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:09,977] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:10,139] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:10,331] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:10,579] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:10,769] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:11,006] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:11,185] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:11,383] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:11,607] INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:18,365] INFO HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "[2026-02-11 23:18:18,373] INFO 127.0.0.1 - \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "BASE_DIR = Path('/home/wencm/RecetONA')\n",
    "FRONTEND_DIR = BASE_DIR / 'frontend'\n",
    "RAG_SERVER_SCRIPT = BASE_DIR / 'local_rag_server.py'\n",
    "RAG_HEALTH_URL = 'http://127.0.0.1:8787/health'\n",
    "CHAT_URL = 'http://127.0.0.1:8080/main.html'\n",
    "\n",
    "\n",
    "def _wait_http(url, timeout=30):\n",
    "    end = time.time() + timeout\n",
    "    while time.time() < end:\n",
    "        try:\n",
    "            with urllib.request.urlopen(url, timeout=2) as resp:\n",
    "                if 200 <= resp.status < 500:\n",
    "                    return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.5)\n",
    "    return False\n",
    "\n",
    "\n",
    "def _spawn_once(global_name, cmd, cwd):\n",
    "    proc = globals().get(global_name)\n",
    "    if proc is not None and proc.poll() is None:\n",
    "        return proc, False\n",
    "\n",
    "    proc = subprocess.Popen(cmd, cwd=str(cwd))\n",
    "    globals()[global_name] = proc\n",
    "    return proc, True\n",
    "\n",
    "\n",
    "rag_proc, rag_started = _spawn_once(\n",
    "    'recetona_rag_api_proc',\n",
    "    [sys.executable, str(RAG_SERVER_SCRIPT), '--host', '127.0.0.1', '--port', '8787'],\n",
    "    BASE_DIR,\n",
    ")\n",
    "\n",
    "if not _wait_http(RAG_HEALTH_URL, timeout=45):\n",
    "    raise RuntimeError('No se pudo iniciar la API local RAG en http://127.0.0.1:8787')\n",
    "\n",
    "front_proc, front_started = _spawn_once(\n",
    "    'recetona_frontend_http_proc',\n",
    "    [sys.executable, '-m', 'http.server', '8080', '--bind', '127.0.0.1'],\n",
    "    FRONTEND_DIR,\n",
    ")\n",
    "\n",
    "if not _wait_http(CHAT_URL, timeout=20):\n",
    "    raise RuntimeError('No se pudo iniciar el frontend local en http://127.0.0.1:8080/main.html')\n",
    "\n",
    "if rag_started:\n",
    "    print('API RAG local iniciada en: http://127.0.0.1:8787')\n",
    "else:\n",
    "    print('API RAG local ya estaba iniciada.')\n",
    "\n",
    "if front_started:\n",
    "    print('Frontend local iniciado en: http://127.0.0.1:8080/main.html')\n",
    "else:\n",
    "    print('Frontend local ya estaba iniciado.')\n",
    "\n",
    "webbrowser.open(CHAT_URL)\n",
    "display(HTML(f'<a href=\"{CHAT_URL}\" target=\"_blank\">Abrir chat RecetONA</a>'))\n",
    "print('Chat abierto en el navegador.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
